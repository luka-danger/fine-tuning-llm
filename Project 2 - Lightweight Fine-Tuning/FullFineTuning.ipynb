{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Full Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* Model: DistilBERT - I chose DistilBERT because it is a lightweight model that runs quickly and does not require changing all parameters. \n",
    "* Evaluation approach: For evaluation, I compared the accuracy of the model trainer to the accuracy of a fully fine-tuned model. \n",
    "* Fine-tuning dataset: https://huggingface.co/datasets/victor/real-or-fake-fake-jobposting-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.5.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement transfomers (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for transfomers\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install peft transfomers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd748d8bd554312855699013e2dbcaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57206008f91341019e32c537c5187b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/50.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f589f20b35064ae1bc0b75b06414b5b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/17880 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['job_id', 'title', 'location', 'department', 'salary_range', 'company_profile', 'description', 'requirements', 'benefits', 'telecommuting', 'has_company_logo', 'has_questions', 'employment_type', 'required_experience', 'required_education', 'industry', 'function', 'fraudulent'],\n",
      "        num_rows: 17433\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['job_id', 'title', 'location', 'department', 'salary_range', 'company_profile', 'description', 'requirements', 'benefits', 'telecommuting', 'has_company_logo', 'has_questions', 'employment_type', 'required_experience', 'required_education', 'industry', 'function', 'fraudulent'],\n",
      "        num_rows: 447\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35874dd4386549669102edf1fd83c799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17433 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7c5ca101b54fc8aeffe0d20481891c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['job_id', 'title', 'location', 'department', 'salary_range', 'company_profile', 'text', 'requirements', 'benefits', 'telecommuting', 'has_company_logo', 'has_questions', 'employment_type', 'required_experience', 'required_education', 'industry', 'function', 'labels', 'input_ids', 'attention_mask']\n",
      "['job_id', 'title', 'location', 'department', 'salary_range', 'company_profile', 'text', 'requirements', 'benefits', 'telecommuting', 'has_company_logo', 'has_questions', 'employment_type', 'required_experience', 'required_education', 'industry', 'function', 'labels', 'input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# Import modules and packages\n",
    "import torch \n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load Real or Fake Jobposting Dataset\n",
    "# URL: https://huggingface.co/datasets/victor/real-or-fake-fake-jobposting-prediction\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Split dataset into training and testing datasets\n",
    "# Use seed as a repeatable constant for Shuffle \n",
    "# Use 2.5% of data for tests\n",
    "# Attribution: https://huggingface.co/docs/datasets/en/process\n",
    "\n",
    "dataset = load_dataset(\"victor/real-or-fake-fake-jobposting-prediction\", \n",
    "    split=\"train\").train_test_split(test_size=0.025, shuffle=True, seed=17)\n",
    "\n",
    "# Split datasets into train (full dataset) and test (2.5% of dataset)\n",
    "split_datasets = [\"train\", \"test\"]\n",
    "\n",
    "# For partial fine-tuning \n",
    "# Use first 500 in training dataset\n",
    "#dataset[\"train\"] = dataset[\"train\"].select(range(500))\n",
    "\n",
    "# View Datasets\n",
    "print(dataset)\n",
    "\n",
    "# Load Tokenizer from Pre-trained distilbert model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Create tokenizer function \n",
    "# Attribution: Udacity \"Adapting Foundation Models\"\n",
    "def tokenizer_function(examples):\n",
    "    return tokenizer(examples[\"title\"], padding=\"max_length\", truncation = True)\n",
    "\n",
    "\n",
    "# Tokenize Train and Test Datasets \n",
    "tokenize_datasets = {}\n",
    "for set in split_datasets:\n",
    "    tokenize_datasets[set] = dataset[set].map(tokenizer_function, batched=True)\n",
    "\n",
    "# Rename columns for text classification\n",
    "# Attribution: https://stackoverflow.com/questions/73290491/the-model-did-not-return-a-loss-from-the-inputs-labse-error\n",
    "tokenize_datasets[\"train\"] = tokenize_datasets[\"train\"].rename_column(\"description\", \"text\")\n",
    "tokenize_datasets[\"train\"] = tokenize_datasets[\"train\"].rename_column(\"fraudulent\", \"labels\")\n",
    "tokenize_datasets[\"test\"] = tokenize_datasets[\"test\"].rename_column(\"description\", \"text\")\n",
    "tokenize_datasets[\"test\"] = tokenize_datasets[\"test\"].rename_column(\"fraudulent\", \"labels\")\n",
    "\n",
    "    \n",
    "# Convert Model Predictions to Strings / Convert Labels to Integers\n",
    "# Attribution: Udacity \"Adapting Foundation Models\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    # Create Real and Fake Labels\n",
    "    num_labels=2,\n",
    "    id2label={0: \"REAL\", 1: \"FAKE\"},  \n",
    "    label2id={\"REAL\": 0, \"FAKE\": 1},\n",
    ")\n",
    "\n",
    "# Unfreeze Parameters of Base Model\n",
    "# Attribution: https://huggingface.co/transformers/v4.2.2/training.html\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "    \n",
    "print(tokenize_datasets[\"train\"].column_names)\n",
    "print(tokenize_datasets[\"test\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# View model \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset:\n",
      "\n",
      "Dataset({\n",
      "    features: ['job_id', 'title', 'location', 'department', 'salary_range', 'company_profile', 'text', 'requirements', 'benefits', 'telecommuting', 'has_company_logo', 'has_questions', 'employment_type', 'required_experience', 'required_education', 'industry', 'function', 'labels', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 17433\n",
      "})\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Testing Dataset:\n",
      "\n",
      "Dataset({\n",
      "    features: ['job_id', 'title', 'location', 'department', 'salary_range', 'company_profile', 'text', 'requirements', 'benefits', 'telecommuting', 'has_company_logo', 'has_questions', 'employment_type', 'required_experience', 'required_education', 'industry', 'function', 'labels', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 447\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# View Training Datasets\n",
    "print(\"Training Dataset:\\n\")\n",
    "print(tokenize_datasets[\"train\"])\n",
    "\n",
    "print(\"\\n--------------------------------------------------\\n\")\n",
    "\n",
    "print(\"Testing Dataset:\\n\")\n",
    "# View Test Datasets\n",
    "print(tokenize_datasets[\"test\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/accelerate/accelerator.py:447: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3487' max='3487' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3487/3487 05:14, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.194300</td>\n",
       "      <td>0.197606</td>\n",
       "      <td>0.941834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [90/90 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "import numpy \n",
    "\n",
    "\n",
    "# Compute accuracy of model \n",
    "# Attribution: https://huggingface.co/docs/transformers/main_classes/trainer\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = numpy.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "# Configure Model Trainer \n",
    "# Attribution: https://huggingface.co/docs/transformers/main_classes/trainer\n",
    "\n",
    "trainer_model = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/job_results_model\",\n",
    "        # Overwrite saved data each time code runs\n",
    "        overwrite_output_dir=True,\n",
    "        learning_rate=2e-2,\n",
    "        per_device_train_batch_size=5,\n",
    "        per_device_eval_batch_size=5,\n",
    "        # Run a training loop w/ each epoch \n",
    "        evaluation_strategy=\"epoch\",\n",
    "        # Save the trained model\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "    ),\n",
    "    train_dataset=tokenize_datasets[\"train\"],\n",
    "    eval_dataset=tokenize_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer_model.train()\n",
    "\n",
    "# Evaluate the model\n",
    "model_eval = trainer_model.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/accelerate/accelerator.py:447: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2906' max='2906' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2906/2906 28:24, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.164800</td>\n",
       "      <td>0.166906</td>\n",
       "      <td>0.953020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.124500</td>\n",
       "      <td>0.152898</td>\n",
       "      <td>0.964206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Unfreeze model parameters prior to fine-tuning\n",
    "# Attribution: Udacity \"Adapting Foundation Models\"\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Configure PEFT Trainer\n",
    "# Changes: Decrease learning rate, increase batch size, increase number of epochs\n",
    "trainer_full_tuned = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        # Create job_results_peft directory\n",
    "        # Output training data to created directory\n",
    "        output_dir=\"./data/job_results_full_fine_tuned\",\n",
    "        # Overwrite saved data each time code runs\n",
    "        overwrite_output_dir=True,\n",
    "        # Decrease learning rate to allow gradual fine-tuning \n",
    "        # Increase liklihood of accurate predictions \n",
    "        learning_rate=2e-5,\n",
    "        # Increase batch size to provide more accurate prediction\n",
    "        per_device_train_batch_size=12,\n",
    "        per_device_eval_batch_size=12,\n",
    "        # Run a training loop w/ each epoch \n",
    "        evaluation_strategy=\"epoch\",\n",
    "        # Save the trained model at each epoch\n",
    "        save_strategy=\"epoch\",\n",
    "        # Increase epochs to allow more iterations for training\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "    ),\n",
    "    train_dataset=tokenize_datasets[\"train\"],\n",
    "    eval_dataset=tokenize_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the PEFT\n",
    "trainer_full_tuned.train()\n",
    "\n",
    "# Evaluate the PEFT\n",
    "full_tuned_eval = trainer_full_tuned.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Results: \n",
      "{'eval_loss': 0.1976059377193451, 'eval_accuracy': 0.941834451901566, 'eval_runtime': 7.2759, 'eval_samples_per_second': 61.436, 'eval_steps_per_second': 12.37, 'epoch': 1.0}\n",
      "\n",
      "\n",
      "Fine-Tuned Model Results: \n",
      "{'eval_loss': 0.15289801359176636, 'eval_accuracy': 0.9642058165548099, 'eval_runtime': 8.0982, 'eval_samples_per_second': 55.197, 'eval_steps_per_second': 4.692, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "# Compare results of Model and PEFT\n",
    "\n",
    "print(\"Base Model Results: \")\n",
    "# Evaluate the model\n",
    "print(model_eval) \n",
    "\n",
    "# Access Eval Accuracy from Model Evaluation \n",
    "model_accuracy = model_eval['eval_accuracy']\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Fine-Tuned Model Results: \")\n",
    "# Evaluate the PEFT\n",
    "print(full_tuned_eval)\n",
    "\n",
    "# Access Eval Accuracy from PEFT Evaluation \n",
    "full_tuned_accuracy = full_tuned_eval['eval_accuracy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Fine-Tuning improved prediction accuracy by 2.24%!\n"
     ]
    }
   ],
   "source": [
    "if full_tuned_accuracy > model_accuracy:\n",
    "    tuned_improvement = (full_tuned_accuracy - model_accuracy) * 100\n",
    "    print('Full Fine-Tuning improved prediction accuracy by {:.2f}%!'.format(tuned_improvement))\n",
    "else: \n",
    "    print(\"No improvement with fine-tuning. Additional training is recommended.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
